---
title: "Assignment 2 Task 2"
author: "Amelia Ritger"
date: "3/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message=FALSE,
                      warning=FALSE)
```

For this assignment, I will be performing a text analysis of Rachel Carson's "Silent Spring". (For more information, [check out the wikipedia page](https://en.wikipedia.org/wiki/Silent_Spring)) I will be creating a word cloud of the most commonly used words, as well as performing a sentiment analysis to identify overall sentiments in the text (though, with an understanding of the book, we can expect the majority sentiment to be negative). 

Load packages and data
```{r}
library(tidyverse)
library(here)
library(pdftools)
library(tidytext)
library(textdata)
library(ggwordcloud)

path <- here("Silent_Spring-Rachel_Carson-1962.pdf")
spring_text <- pdftools::pdf_text(path)
```

Do some wrangling
```{r}
spring <- data.frame(spring_text) %>%
  mutate(text_full = str_split(spring_text, pattern = "\r\n")) %>%  #split up existing strings in rows, at each line break >> each line shows up as a difference piece of a string
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) %>% #get rid of empty space endpoints
  unnest_tokens(word,text_full) #give every word its own row
```

How many of each word are in the book?
```{r}
spring_wc <- spring %>% 
  count(word) %>% 
  arrange(-n)
```

Remove stop words
```{r}
spring_stop <- spring %>% 
  anti_join(stop_words) %>% #remove stopwords from ipcc_token
  dplyr::select(-spring_text)
```

Remove all numeric pieces:
```{r}
spring_no_numeric <- spring_stop %>% 
  dplyr::filter(is.na(as.numeric(word))) #for every entry in "word" column, convert it to a number - if it's not a number, return NA
```

### Start doing some visualization

# Here's a word cloud!
```{r}
spring_top100 <- spring_no_numeric %>%
  mutate(word = str_replace_all(word,c("[^[:alnum:]]$" = "",  "s$" = "", "(\\(\\d*)" = "\\1\\)" ))) %>% #combine plural and singular words
  count(word) %>% 
  arrange(-n) %>% 
  head(100) 

ggplot(data=spring_top100, aes(label=word, size=n)) +
  geom_text_wordcloud(aes(color=n), shape="diamond") +
  scale_size_area(max_size=12) +
  scale_color_gradientn(colors = c("darkgreen", "blue", "red")) +
  theme_minimal()
```

It comes at no surprise that among the most commonly used words in Rachel Carson's Silent Spring are chemical, insecticide, spraying, and ddt. These are found 467, 234, 190, and 188 times, respectively. 

Sentiment analysis (just a wild guess, this is going to be generally negative...)
```{r}
spring_afinn <- spring_stop %>% 
  inner_join(get_sentiments(lexicon="afinn"))

spring_afinn_hist <- spring_afinn %>% 
  count(value)

ggplot(data=spring_afinn_hist, aes(x=value, y=n)) +
  geom_col() +
  theme_minimal()
```

```{r}
spring_afinn2 <- spring_afinn %>% 
  filter(value==2) #all words associated with value of 2

spring_summary <- spring_afinn %>% 
  summarize(
    mean_score = mean(value), 
    median_score = median(value)
  )
```

Check out sentiments by NRC
```{r}
spring_nrc <- spring_stop %>% 
  inner_join(get_sentiments(lexicon = "nrc"))

# See what's excluded:
spring_exclude <- spring_stop %>% 
  anti_join(get_sentiments(lexicon="nrc"))
```

## Find counts by sentiment:
```{r}
spring_nrc_n <- spring_nrc %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(sentiment=as.factor(sentiment),
         sentiment=fct_reorder(sentiment, -n)) #make it ggplot friendly (AKA remove default alphabetical order)

ggplot(data=spring_nrc_n) +
  geom_col(aes(x=sentiment, y=n)) +
  labs(y="Count", x="Sentiment") +
  theme_minimal()
```

Performing sentiment analysis using [NRC Emotion Lexicon](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm), we see that there is about equivalent postiive and negative words. However, among the top 5 most common sentiments, trust is the only positive one of the bunch.

For each sentiment bin, what are the top 5 most frequent words associated with that bin?
```{r}
spring_nrc_n5 <- spring_nrc %>% 
  count(word, sentiment, sort=TRUE) %>% #get counts for each word associated for each sentiment
  group_by(sentiment) %>% 
  top_n(5) %>% #EXPECTS YOU TO HAVE DONE COUNT() ALREADY; get top 5 most common words for each group; ties are included in top_n()
  ungroup()

ggplot(data=spring_nrc_n5, aes(x=reorder(word,n), y=n), fill=sentiment) + #here's another way to reorder characters!
  geom_col(show.legend=FALSE, aes(fill=sentiment)) +
  facet_wrap(~sentiment, ncol=2, scales="free")
